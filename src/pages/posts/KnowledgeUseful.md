---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Thoughts: Will we still need to know stuff?'
pubDate: 2023-06-17
description: 'With language models emerging, do I still need to learn how to program?'
author: 'Stanislaw Kalinowski'
color: 'F1C550'
---
With language models existing, I've been seeing people question the need for learning. Learning how to program, write, etc. And I think learning is still going to be important, it's just going to change.

Similar to the Socrates argument against writing, where writing things down makes us lazy. It's true, we memorize less now, but we still remember things! We memorize what needs to be memorized, and with language models, we will learn what needs to be learned!

And what will need to be learned?

Well if the model could take care of 90%, heck 99% of knowledge tasks, there is still a need of being informed:

→Knowledge helps us know what to want.

→Knowledge helps us use the model.

→Knowledge helps us cover up for the model.

************************************************************************Knowledge helps us know what to want.************************************************************************

These language models will be put to work, but they still work for us. They will output what we want. Being informed helps us make decisions on what we want, what to actually use the models for. It helps us be more specific with our intentions and desires.

**Knowledge helps us use the model.**

It helps us use the model. In the sense of relaxing the requirements of the model. Think about how asking it to explain a topic, there are a variety of levels and way to explain a topic. Just using basic fifth grader language will require a lot of text, by being more knowledgeable the model can be more efficient and give a clear explanations. Our background knowledge effects the output of the model in this sense, what analogies and language it can use. 

************************************************************************************Knowledge helps us cover up for the model.************************************************************************************

I think this is the most important part, similar to how we offload what we memorize to paper, we will off load what we need to learn onto models. Therefore, what we will learn most is where the models lack.

Currently, the most prevalent lacking part is the out of scope, or areas away from the training data. Sure, they can ideate and generate suggestions, maybe even make discoveries, but this is still something we are figuring out, and remains an open question. 

Therefore, we will learn how to explore, how to innovate, how to understand new knowledge. 

But, even for normal task, there is still abilities which are lacking! When I use GPT4 to program, it can give me good starting points and suggestion, but I find myself carrying the other parts. There is a “last mile” problem with language models.

Even as they get better, I believe some level of of this problem will remain due to one factor, language is naturally ambiguous.  So even when we specify our context and desires fully, some part of it will be ambiguous and it will generate some wrong parts. We will need to recognize and correct it.

********************Conclusion********************

Therefore I think that being informed and knowledge is still important, even if the models will be taking care of 90% of the job. The knowledge helps us unlock the models. Allowing for better usage of the models, better specification, and for better use of the output through modifying & correcting it.

I want to end with a bit of a diverge, and discuss why knowing things is important because of one factor, knowledge is compounding. There is a great book called Prost & the Squid. It's about how children learn and acquire the ability to read. I liked their description of this learning process. Initially kids struggle and trudge through the text, they just don't know the words. But, suddenly, once they pass some threshold of knowledge, they acquire the ability to read. They can fill in the blanks for the words they don't know through the context. 

In my mind, knowledge works like this, it's a compounding process, the more we know, the easier it become to know more.